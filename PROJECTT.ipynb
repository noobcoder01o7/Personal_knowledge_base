{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649c798a-00f8-4b76-8064-c3e15b990fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain langchain-core langchain_community langchain_ollama langchain_text_splitters langchain_chroma streamlit\n",
    "#!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2019d779-5357-4a97-925c-7941a5a37ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f6a1fb9-565c-4b4c-ba2e-cbb819f50932",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"documents\"\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "# FIX: Define models clearly\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\" \n",
    "LLM_MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0992c24-2bb6-426a-aa6a-7e15418369c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 120 document(s).\n",
      "Split documents into 337 chunks.\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(DATA_PATH, glob=\"**/*.pdf\", loader_cls=PyPDFLoader, silent_errors=True)\n",
    "documents = loader.load()\n",
    "\n",
    "if not documents:\n",
    "    print(f\"No PDF documents found in '{DATA_PATH}'. Please add your notes.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(documents)} document(s).\")\n",
    "    \n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split documents into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d127b047-2c96-4953-8175-3ba7a68c27e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings with 'nomic-embed-text' and storing in Chroma... (This may take a moment)\n",
      "Successfully saved 337 chunks to Chroma DB at 'chroma'.\n"
     ]
    }
   ],
   "source": [
    "def ingest_documents():\n",
    "    if not 'chunks' in globals() or not chunks:\n",
    "        print(\"Error: No chunks found. Please run the 'Load and Split' cell first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Creating embeddings with '{EMBEDDING_MODEL}' and storing in Chroma... (This may take a moment)\")\n",
    "    \n",
    "    # FIX: Use the fast embedding model\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks, \n",
    "        embedding=embeddings, \n",
    "        persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    print(f\"Successfully saved {len(chunks)} chunks to Chroma DB at '{CHROMA_PATH}'.\")\n",
    "\n",
    "# Run the ingestion\n",
    "ingest_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6719a9a-a564-4e99-b5cb-fa7589caa40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(question_text: str):\n",
    "    print(f\"Preparing to query with '{EMBEDDING_MODEL}' and '{LLM_MODEL}'...\")\n",
    "\n",
    "    # Load embeddings\n",
    "    # FIX: Must use the SAME embedding model used for ingestion\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "    \n",
    "    # Load existing DB\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    # Define prompt\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "    ---\n",
    "    Answer the question based on the above context: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    # Define model\n",
    "    # FIX: Use the LLM for generating the final answer\n",
    "    model = OllamaLLM(model=LLM_MODEL)\n",
    "\n",
    "    # Define RAG chain\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"Querying the AI...\")\n",
    "    response = chain.invoke(question_text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c7dbd2-5f09-427d-a21c-9dc4ebc8d342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to query with 'nomic-embed-text' and 'llama3'...\n",
      "Querying the AI...\n",
      "\n",
      "--- AI Answer ---\n",
      "\n",
      "According to the provided context, machine learning is defined as \"programming computers to optimize a performance criterion using example data or past experience.\" It is also described as \"the field of study that gives computers the ability to learn without being explicitly programmed\" by Arthur Samuel in 1959.\n"
     ]
    }
   ],
   "source": [
    "my_question = \"What is Machine learning?\" \n",
    "answer = query_rag(my_question)\n",
    "\n",
    "print(\"\\n--- AI Answer ---\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2f8eb-17af-49a2-b892-4be7ab0b64b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f170fb05-e098-4b29-9562-a47efdbb78c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e43bee-a269-4627-b637-df9635c6a264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee83b8b-05d3-41d5-b994-e77685df8d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bbfab-f6d9-415a-9f8b-225adc904229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
